# Concepts

## Attention

* Attn: Illustrated Attention. (2023). Retrieved 29 January 2023, from https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458
  * 大师兄​澳门大学 计算机科学硕士. Attention 图解 - 知乎. (2023). Retrieved 29 January 2023, from https://zhuanlan.zhihu.com/p/342235515
* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [1706.03762] Attention Is All You Need. (2023). Retrieved 29 January 2023, from https://arxiv.org/abs/1706.03762
  * 习翔宇. 论文解读:Attention is All you need - 知乎. (2023). Retrieved 29 January 2023, from https://zhuanlan.zhihu.com/p/46990010

## Transformer

* The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. (2023). Retrieved 29 January 2023, from http://jalammar.github.io/illustrated-transformer/

### Transformer 实现

* Kyubyong. transformer. (2023). Retrieved 29 January 2023, from https://github.com/Kyubyong/transformer


## BERT

* Jeffery深度学习小白白. 什么是BERT？ - 知乎. (2023). Retrieved 29 January 2023, from https://zhuanlan.zhihu.com/p/98855346
* Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (2023). Retrieved 29 January 2023, from https://arxiv.org/abs/1810.04805